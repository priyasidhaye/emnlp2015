\section{Results}

As seen in the results of the analyses performed in \secref{sec:analysis}, the tweets have little in common with the articles they are related to. The analyses are based on the ROUGE-1,2 and L like calculations. This shows that extractive summarization algorithms cannot be directly applied to articles to generate tweets. 

To tie in the results of the findings above with some intuitive notions about the text and see how formality interacts with the results, we also calculated the formality of the articles. This formality score was correlated with the longest common subsequence. To achieve this, the degree of formality of the text was calculated with the help of \newcite{brooke2013multi}. The formality lexicon was generated by analyzing the stylistics of text and can be used to measure formality of a given text. They calculate formality scores for words and sentences by training a model on a large corpus based on the appearance of words in specific documents. Their model represents words as vectors and the formal and informal seeds appear in opposite halves of the graphs, suggesting that we can use these seeds to determine if an article is formal or informal. The lexicon consists of words and phrases and the degree of formality for their occurrence. Thus, more formal words are marked on a positive scale and informal words like those occurring in colloquial language are marked on a negative scale. The degree of formality was calculated using this lexicon.

\begin{equation}
\resizebox{.9\hsize}{!}{$formalityScore = \frac{| unigramsArticle \cap formalitySet |}{| unigramsArticle |} * 10$}
\end{equation}

The formality lexicon gave positive weights for formal expressions and negative for informal expressions. After calculating the formality weights for all articles, it was observed that they all had a total negative normalized weight, meaning a lot more informal expressions were getting matched. Hence, we used just the formal word occurrences for calculating the weight. Thus, above a certain cut-off weight, the article could be considered formal, else would be considered informal. To make sure these formality scores intuitively made sense, we calculated the average formality score for each hashtag used in the search during data extraction and ordered them, shown in \tabref{tab:formal}


\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
Lowest  & Highest  \\ \hline
\#theforceawakens       & \#KevinVickers           \\
\#TaylorSwift           & \#erdogan                \\
\#winteriscoming        & \#apec                  \\ \hline
\end{tabular}
\captionof{table}{Table of hashtags(broadly, topics) with highest and lowest formality according to the lexicon.}
\label{tab:formal}
\end{table}

This formality score for each article was then correlated with the percentage of match obtained using the longest common subsequence algorithm. The Pearson correlation value was 0.41, with a p-value of 7.08e-66. The p-value justifies that we can reject the null hypothesis, and say with confidence that there is a correlation between the formality scores and the ROUGE-L scores of the tweets and articles. Hence, we can say that the more formal the subject or the article, there are higher chances of the tweet being extracted directly from the article.