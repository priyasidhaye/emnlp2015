\section{Background and Related Work}
\label{sec:background}

There have been studies on a number of different issues related to Twitter data, including classifying tweets and sentiment analysis of tweets. \newcite{ghosh2011entropy} classified the retweeting activity of users based on time intervals between retweets of a single user and frequency of retweets from unique users. `Retweet' here means the occurrence of the same URL in a different tweet. The study was able to classify the retweeting as automatic or robotic retweeting, campaigns, news, blogs and so on, based on the time-interval and user-frequency distributions. In another study, \newcite{chen2012extracting} were able to extract sentiment expressions from a corpus of tweets including both formal words and informal slang that bear sentiment.

Other studies using Twitter data include \newcite{o2010tweetmotif}, who use topic summarization for a given search for better browsing. \newcite{chakrabarti2011event} generate an event summary by learning about the event using a Hidden Markov Model over the tweets describing it. \newcite{wang2014socially} generate a coherent event summary by treating summarization as an optimization problem for topic cohesion. \newcite{inouye2011comparing} compare multiple summarization techniques to generate a summary of multi-post blogs on Twitter.

As described in \secref{sec:intro}, we analyze tweet generation using measures inspired by extractive summarization evaluation. There has been one study comparing different text summarization techniques for tweet generation by \newcite{lloret2013towards}. Summarization systems were used to summarize texts into sentences, where were then taken to be tweets. The system-generated tweets were evaluated using ROUGE measures \cite{lin2004rouge}. The ROUGE-1, ROUGE-2 and ROUGE-L measures were used, and a human-written reference tweet was taken to be the gold standard. %ROUGE has been known to work better when multiple reference summaries are used and is not meant to be used at the sentence level. This study uses ROUGE with a single reference summary, which is the reference tweet. However, given the size of a tweet, it can be argued that while generating a reference tweet from a single document, it is difficult to generate multiple reference tweets with largely varying content. \unsure{Is this reasoning okay?}
\todo{The sentences about the number of references required for ROUGE are interesting, but they seemed like a digression, so I took it out. If you'd like to add it back in, please shorten it and make it very succinct.}

The limits of extractive summarization have been studied by \newcite{he2000comparing}. They compare user preferences for various types of summaries of an audio-visual presentation. They demonstrate that the most preferred method of summarization is highlights and notes provided by the author, rather than transcripts or slides from the presentation. \newcite{conroy2006topic} computed an oracle ROUGE score to investigate the same issue of the limits of extraction. 
%The oracle score is based on the maximum likelihood probability of words occurring in model summaries and is in turn used to generate summaries that perform better than any extracted and also human-generated summaries.
\todo{What you said about the oracle score is actually about the approximate oracle score they defined for their system.} 
These studies show that extractive summarization algorithms may not generate good quality summaries despite giving high ROUGE evaluation scores. \newcite{cheung2013towards} show that for the news genre, extractive summarization systems that are optimized for centrality---that is, getting the core parts of the text into the summary---cannot perform well when compared to model summaries, since the model summaries are abstracted from the document to a large extent.

