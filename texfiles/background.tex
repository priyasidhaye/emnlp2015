\section{Background and Related Work}
\label{sec:background}

There have been studies on a number of different issues related to Twitter data, including classifying tweets and sentiment analysis of tweets. \newcite{ghosh2011entropy} classified the retweeting activity of users based on time intervals between retweets of a single user and frequency of retweets from unique users. 'Retweet' in this context was considered to be the occurrence of the same URL in a different tweet. The study was able to classify the retweeting as automatic or robotic retweeting, campaigns, news, blogs and so on, based on the time-interval and user-frequency distributions. In another study, \newcite{chen2012extracting} were able to extract sentiment expressions from a corpus of tweets including both formal words and informal slang that bear sentiment.

Other studies using Twitter data include \newcite{o2010tweetmotif}, who use topic summarization for a given search for better browsing. \newcite{chakrabarti2011event} generate an event summary by learning about the event using a Hidden Markov Model over the tweets describing it. \newcite{wang2014socially} generate a coherent event summary by treating summarization as an optimization problem for topic cohesion. \newcite{inouye2011comparing} compare multiple summarization techniques to generate a summary of multi-post blogs on Twitter.

As described in \secref{sec:intro}, we analyze tweet generation using measures inspired by extractive summarization evaluation. There has been one such study comparing different text summarization techniques for tweet generation by  \newcite{lloret2013towards}. Summarization systems were used to summarize texts to sentences and then were compared against each other, evaluated using the ROUGE metric for evaluation. The ROUGE-1, ROUGE-2 and ROUGE-L metrics were used and the tweets were compared against an ideal summary. ROUGE \cite{lin2004rouge} is a recall based n-gram counting evaluation metric developed for summarization \cite{nenkova2006summarization}. ROUGE has been known to work better when multiple reference summaries are used and is not meant to be used at the sentence level. This study uses ROUGE with a single reference summary, which is the reference tweet. However, given the size of a tweet, it can be argued that while generating a reference tweet from a single document, it is difficult to generate multiple reference tweets with largely varying content. \unsure{Is this reasoning okay?}

The limits of extractive summarization have been studied by \newcite{he2000comparing} by comparing user preferences for multiple types of summaries for an audio-visual presentation. They demonstrate that the most preferred method of summarization is highlights and notes provided by the author, rather than transcripts or slides from the presentation. \newcite{conroy2006topic} have defined an oracle score to achieve the same aim. The oracle score is based on the maximum likelihood probability of words occurring in model summaries and is in turn used to generate summaries that perform better than any extracted and also human-generated summaries. These studies show that extractive summarization algorithms may not generate good quality summaries despite giving high ROUGE evaluation scores. \newcite{cheung2013towards} show that extractive summarization systems that are optimized for centrality, that is, getting the core parts of the text into the summary, cannot perform well when compared to model summaries, since the model summaries are abstracted from the document to a large extent.

